# notes
======
#### items are listed by alphabetical order

### Gradient
* Gradient descent
* Gradient vanishing
* Gradient explosion
* Gradient boosting
* Gradient clipping

### Normalization
* Linear/std/nonlinear
* Local response normalization, boost active response 
* Batch normalization, constraint intermediate value within network
* Layer normalization,
* Instance 
* Group normalization, improve robustness in small batches

### Optimizers
* ADAM
* SGD

### Regularization
* L1
* L2
* dropout
